#!/usr/bin/env python3
"""
Verify correctness of the optimized Metal kernel
Compares against PyTorch reference implementation
"""

import sys
import numpy as np

# Initialize Metal with OPTIMIZED kernel
try:
    import _flash_attn_metal
    _flash_attn_metal.initialize()
    
    # Load common and OPTIMIZED kernel
    common_src = open('kernels/common.metal').read()
    opt_src = open('kernels/flash_attention_fwd_optimized.metal').read()
    _flash_attn_metal.compile_shaders(common_src + '\n' + opt_src)
    print("‚úÖ Optimized Metal kernel loaded\n")
except Exception as e:
    print(f"‚ùå Metal initialization failed: {e}")
    sys.exit(1)

# Import PyTorch for reference
try:
    import torch
    import torch.nn.functional as F
    print("‚úÖ PyTorch available\n")
except ImportError:
    print("‚ùå PyTorch not available")
    sys.exit(1)


def torch_attention_reference(q, k, v, softmax_scale, is_causal=False):
    """Reference implementation using PyTorch"""
    # q, k, v: [batch, seqlen, num_heads, head_dim]
    batch, seqlen, num_heads, head_dim = q.shape
    
    # Convert to torch
    q_t = torch.from_numpy(q).float()
    k_t = torch.from_numpy(k).float()
    v_t = torch.from_numpy(v).float()
    
    # Reshape to [batch, num_heads, seqlen, head_dim]
    q_t = q_t.transpose(1, 2)
    k_t = k_t.transpose(1, 2)
    v_t = v_t.transpose(1, 2)
    
    # Compute attention: S = Q @ K^T * scale
    scores = torch.matmul(q_t, k_t.transpose(-2, -1)) * softmax_scale
    
    # Apply causal mask if needed
    if is_causal:
        mask = torch.triu(torch.ones(seqlen, seqlen), diagonal=1).bool()
        scores = scores.masked_fill(mask, float('-inf'))
    
    # Softmax
    attn = F.softmax(scores, dim=-1)
    
    # Output: O = Attention @ V
    output = torch.matmul(attn, v_t)
    
    # Reshape back to [batch, seqlen, num_heads, head_dim]
    output = output.transpose(1, 2)
    
    return output.numpy().astype(np.float16)


def test_correctness(batch, seqlen, num_heads, head_dim, is_causal=False):
    """Test one configuration"""
    
    # Generate inputs
    np.random.seed(42)
    q = np.random.randn(batch, seqlen, num_heads, head_dim).astype(np.float16) * 0.02
    k = q.copy()
    v = q.copy()
    softmax_scale = 1.0 / np.sqrt(head_dim)
    
    # Metal output
    metal_out = _flash_attn_metal.forward(q, k, v, softmax_scale=softmax_scale, is_causal=is_causal)
    
    # PyTorch reference output
    torch_out = torch_attention_reference(q, k, v, softmax_scale, is_causal)
    
    # Compare
    diff = np.abs(metal_out.astype(np.float32) - torch_out.astype(np.float32))
    max_diff = np.max(diff)
    mean_diff = np.mean(diff)
    
    # Check for NaN or Inf
    has_nan = np.any(np.isnan(metal_out))
    has_inf = np.any(np.isinf(metal_out))
    
    return max_diff, mean_diff, has_nan, has_inf


def main():
    print("=" * 80)
    print("üîç CORRECTNESS VERIFICATION: Optimized Metal Kernel")
    print("=" * 80)
    print()
    
    # Test configurations
    configs = [
        (1, 128, 8, 64, False, "Small non-causal"),
        (1, 256, 8, 64, False, "Medium non-causal"),
        (1, 512, 8, 64, False, "Large non-causal"),
        (1, 1024, 8, 64, False, "XLarge non-causal"),
        (2, 512, 8, 64, False, "Batch=2 non-causal"),
        (1, 512, 4, 64, False, "4 heads non-causal"),
        (1, 512, 16, 64, False, "16 heads non-causal"),
        (1, 512, 8, 128, False, "128-dim non-causal"),
        (1, 256, 8, 64, True, "Medium causal"),
        (1, 512, 8, 64, True, "Large causal"),
    ]
    
    all_passed = True
    results = []
    
    for batch, seqlen, num_heads, head_dim, is_causal, desc in configs:
        print(f"Testing: {desc:30} [{batch}x{seqlen}x{num_heads}x{head_dim}]")
        
        max_diff, mean_diff, has_nan, has_inf = test_correctness(
            batch, seqlen, num_heads, head_dim, is_causal
        )
        
        # Tolerance for fp16: ~0.01 is acceptable
        passed = max_diff < 0.01 and not has_nan and not has_inf
        
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        if not passed:
            all_passed = False
        
        print(f"  Max diff: {max_diff:.6f} | Mean diff: {mean_diff:.6f} | {status}")
        
        if has_nan:
            print(f"  ‚ö†Ô∏è  WARNING: NaN detected!")
        if has_inf:
            print(f"  ‚ö†Ô∏è  WARNING: Inf detected!")
        
        print()
        
        results.append({
            'desc': desc,
            'max_diff': max_diff,
            'mean_diff': mean_diff,
            'passed': passed,
            'has_nan': has_nan,
            'has_inf': has_inf
        })
    
    # Summary
    print("=" * 80)
    print("üìä SUMMARY")
    print("=" * 80)
    print()
    
    passed_count = sum(1 for r in results if r['passed'])
    total_count = len(results)
    
    print(f"{'Configuration':<32} {'Max Error':>12} {'Mean Error':>12} {'Status':>10}")
    print("-" * 80)
    
    for r in results:
        status = "‚úÖ PASS" if r['passed'] else "‚ùå FAIL"
        print(f"{r['desc']:<32} {r['max_diff']:>12.6f} {r['mean_diff']:>12.6f} {status:>10}")
    
    print()
    print("=" * 80)
    
    if all_passed:
        print(f"‚úÖ ALL TESTS PASSED: {passed_count}/{total_count} ({100*passed_count/total_count:.0f}%)")
        print("‚úÖ Optimized kernel is CORRECT and ready for production!")
    else:
        print(f"‚ùå SOME TESTS FAILED: {passed_count}/{total_count} ({100*passed_count/total_count:.0f}%)")
        print("‚ö†Ô∏è  Optimized kernel needs debugging!")
    
    print("=" * 80)
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
